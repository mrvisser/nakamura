By default, nakamura should have a restrictive /robots.txt policy file. 
This should keep most web crawling spiders out by default.

If you would like the spiders to index your data, then you should update 
/robots.txt with a more appropriate set of permissions.

See:  http://en.wikipedia.org/wiki/Robots_exclusion_standard
